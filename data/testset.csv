question,contexts,ground_truth,evolution_type,metadata,episode_done
How can understanding the virtual memory system help diagnose performance problems in operating systems?,"[' how classic ideas, found in old papers such as this one on VAX/VMS, still inﬂuence how modern operating systems are built.\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nWWW.OSTEP.ORG\n\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n\nReferences\n\n[BB+72] “TENEX, A Paged Time Sharing System for the PDP-10” Daniel G. Bobrow, Jerry D. Burchﬁel, Daniel L. Murphy, Raymond S. Tomlinson Communications of the ACM, Volume 15, March 1972 An early time-sharing OS where a number of good ideas came from. Copy-on-write was just one of those; inspiration for many other aspects of modern systems, including process management, virtual memory, and ﬁle systems are found herein.\n\n[BJ81] “Converting a Swap-Based System to do Paging in an Architecture Lacking Page-Reference Bits” Ozalp Babaoglu and William N. Joy SOSP ’81, December 1981, Paciﬁc Grove, California A clever idea paper on how to exploit existing protection machinery within a machine in order to emulate reference bits. The idea came from the group at Berkeley working on their own version of UNIX, known as the Berkeley Systems Distribution, or BSD. The group was heavily inﬂuential in the development of UNIX, in virtual memory, ﬁle systems, and networking.\n\n[BC05] “Understanding the Linux Kernel (Third Edition)” Daniel P. Bovet and Marco Cesati O’Reilly Media, November 2005 One of the many books you can ﬁnd on Linux. They go out of date quickly, but many of the basics remain and are worth reading about.\n\n[C03] “The Innovator’s Dilemma” Clayton M. Christenson Harper Paperbacks, January 2003 A fantastic book about the disk-drive industry and how new innovations disrupt existing ones. A good read for business majors and computer scientists alike. Provides insight on how large and successful companies completely fail.\n\n[C93] “Inside Windows NT” Helen Custer and David Solomon Microsoft Press, 1993 The book about Windows NT that explains the system top to bottom, in more detail than you might like. But seriously, a pretty good book.\n\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System” Henry M. Levy, Peter H. Lipman IEEE Computer, Volume 15, Number 3 (March 1982) Read the original source of most of this ma- terial; tt is a concise and easy read. Particularly important if you wish to go to graduate school, where all you do is read papers, work, read some more papers, work more, eventually write a paper, and then work some more. But it is fun!\n\n[RL81] “Segmented FIFO Page Replacement” Rollins Turner and Henry Levy SIGMETRICS ’81 A short paper that shows for some workloads, segmented FIFO can approach the performance of LRU.\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n253\n\nTHREE EASY PIECES\n\nSummary Dialogue on Memory Virtualization\n\nStudent: (Gulps) Wow, that was a lot of material. Professor: Yes, and? Student: Well, how am I supposed to remember it all? You know, for the exam? Professor: Goodness, I hope that’s not why you are trying to remember it. Student: Why should I then? Professor: Come on, I thought you knew better. You’re trying to learn some- thing here, so that when you go off into the world, you’ll understand how systems actually work. Student: Hmm... can you give an example? Professor: Sure! One time back in graduate school, my friends and I were measuring how long memory accesses took, and once in a while the numbers were way higher than we expected; we thought all the data was ﬁtting nicely into the second-level hardware cache, you see, and thus should have been really fast to access. Student: (nods) Professor: We couldn’t ﬁgure out what was going on. So what do you do in such a case? Easy, ask a professor! So we went and asked one of our professors, who looked at the graph we had produced, and simply said “TLB”. Aha! Of course, TLB misses! Why didn’t we think of that? Having a good model of how virtual memory works helps diagnose all sorts of interesting performance problems. Student:']","Understanding the virtual memory system can help diagnose performance problems in operating systems by providing insights into how memory accesses are managed and how different components interact. For example, knowledge of the translation lookaside buffer (TLB) can explain unexpected delays in memory access times. By understanding the virtual memory system, developers and system administrators can optimize memory usage, identify bottlenecks, and improve overall system performance.",simple,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
How did the VMS designers address the issue of overwhelming memory with page tables in the VAX hardware architecture?,"[' curse was very real, as the VAX-11 architecture was realized in a number of different implementations. Thus, how can an OS be built so as to run effectively on a wide range of systems?\n\n245\n\n23\n\n246\n\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n\nAs an additional issue, VMS is an excellent example of software inno- vations used to hide some of the inherent ﬂaws of the architecture. Al- though the OS often relies on the hardware to build efﬁcient abstractions and illusions, sometimes the hardware designers don’t quite get every- thing right; in the VAX hardware, we’ll see a few examples of this, and what the VMS operating system does to build an effective, working sys- tem despite these hardware ﬂaws.\n\n23.2 Memory Management Hardware\n\nThe VAX-11 provided a 32-bit virtual address space per process, di- vided into 512-byte pages. Thus, a virtual address consisted of a 23-bit VPN and a 9-bit offset. Further, the upper two bits of the VPN were used to differentiate which segment the page resided within; thus, the system was a hybrid of paging and segmentation, as we saw previously.\n\nThe lower-half of the address space was known as “process space” and is unique to each process. In the ﬁrst half of process space (known as P0), the user program is found, as well as a heap which grows downward. In the second half of process space (P1), we ﬁnd the stack, which grows upwards. The upper-half of the address space is known as system space (S), although only half of it is used. Protected OS code and data reside here, and the OS is in this way shared across processes.\n\nOne major concern of the VMS designers was the incredibly small size of pages in the VAX hardware (512 bytes). This size, chosen for historical reasons, has the fundamental problem of making simple linear page ta- bles excessively large. Thus, one of the ﬁrst goals of the VMS designers was to make sure that VMS would not overwhelm memory with page tables.\n\nThe system reduced the pressure page tables place on memory in two ways. First, by segmenting the user address space into two, the VAX-11 provides a page table for each of these regions (P0 and P1) per process; thus, no page-table space is needed for the unused portion of the address space between the stack and the heap. The base and bounds registers are used as you would expect; a base register holds the address of the page table for that segment, and the bounds holds its size (i.e., number of page-table entries).\n\nSecond, the OS reduces memory pressure even further by placing user page tables (for P0 and P1, thus two per process) in kernel virtual mem- ory. Thus, when allocating or growing a page table, the kernel allocates space out of its own virtual memory, in segment S. If memory comes un- der severe pressure, the kernel can swap pages of these page tables out to disk, thus making physical memory available for other uses.\n\nPutting page tables in kernel virtual memory means that address trans- lation is even further complicated. For example, to translate a virtual ad- dress in P0 or P1, the hardware has to ﬁrst try to look up the page-table entry for that page in its page table (the P0 or P1 page table for that pro-\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nWWW.OSTEP.ORG\n\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n\n0\n\nPage 0: Invalid\n\nUser Code\n\nUser Heap\n\nUser (P0)\n\n2 30\n\nUser (P1)\n\nUser Stack\n\n2 31\n\nTrap Tables Kernel Data\n\nKernel Code\n\nSystem (S)\n\nKernel Heap\n\nUnused\n\n2 32\n\nFigure 23.1: The VAX/VMS Address Space\n\ncess); in doing so, however, the hardware may ﬁrst have to consult the system page table (which lives in physical memory); with that transla- tion complete, the hardware can learn the address of the page of the page table, and then ﬁnally learn the address of the desired memory access. All of this, fortunately, is made faster by the VAX’s hardware-managed TLBs, which usually (hopefully) circumvent this laborious lookup.\n\n23.3 A Real Address Space\n\nOne neat aspect of studying VMS is that we can see']","The VMS designers addressed the issue of overwhelming memory with page tables in the VAX hardware architecture by segmenting the user address space into two regions (P0 and P1) per process, providing a page table for each of these regions. This approach eliminated the need for page-table space for the unused portion of the address space between the stack and the heap. Additionally, the user page tables (for P0 and P1) were placed in kernel virtual memory, allowing the kernel to allocate space out of its own virtual memory. If memory pressure became severe, the kernel could swap pages of these page tables out to disk, freeing up physical memory for other uses.",simple,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
How does the file system locate and access an inode during the file read process?,"['bar\n\nroot\n\nfoo\n\nbar\n\nbar\n\nbar\n\nbitmap bitmap inode inode inode data data data[0] data[1] data[1]\n\nread\n\nread\n\nopen(bar)\n\nread\n\nread\n\nread read\n\nread()\n\nread\n\nwrite read\n\nread()\n\nread\n\nwrite read\n\nread()\n\nread\n\nwrite\n\nTable 40.3: File Read Timeline (Time Increasing Downward) the ﬁle system must be able to ﬁnd the inode, but all it has right now is the full pathname. The ﬁle system must traverse the pathname and thus locate the desired inode.\n\nAll traversals begin at the root of the ﬁle system, in the root directory which is simply called /. Thus, the ﬁrst thing the FS will read from disk is the inode of the root directory. But where is this inode? To ﬁnd an inode, we must know its i-number. Usually, we ﬁnd the i-number of a ﬁle or directory in its parent directory; the root has no parent (by deﬁnition). Thus, the root inode number must be “well known”; the FS must know what it is when the ﬁle system is mounted. In most UNIX ﬁle systems, the root inode number is 2. Thus, to begin the process, the FS reads in the block that contains inode number 2 (the ﬁrst inode block).\n\nOnce the inode is read in, the FS can look inside of it to ﬁnd pointers to data blocks, which contain the contents of the root directory. The FS will thus use these on-disk pointers to read through the directory, in this case looking for an entry for foo. By reading in one or more directory data blocks, it will ﬁnd the entry for foo; once found, the FS will also have found the inode number of foo (say it is 44) which it will need next.\n\nThe next step is to recursively traverse the pathname until the desired inode is found. In this example, the FS would next read the block contain- ing the inode of foo and then read in its directory data, ﬁnally ﬁnding the inode number of bar. The ﬁnal step of open(), then, is to read its inode into memory; the FS can then do a ﬁnal permissions check, allocate a ﬁle descriptor for this process in the per-process open-ﬁle table, and return it to the user.\n\nOnce open, the program can then issue a read() system call to read from the ﬁle. The ﬁrst read (at offset 0 unless lseek() has been called) will thus read in the ﬁrst block of the ﬁle, consulting the inode to ﬁnd the location of such a block; it may also update the inode with a new last- accessed time. The read will further update the in-memory open ﬁle table for this ﬁle descriptor, updating the ﬁle offset such that the next read will read the second ﬁle block, etc.\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n471\n\nTHREE EASY PIECES\n\n472\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nFILE SYSTEM IMPLEMENTATION\n\nASIDE: READS DON’T ACCESS ALLOCATION STRUCTURES\n\nWe’ve seen many students get confused by allocation structures such as bitmaps. In particular, many often think that when you are simply reading a ﬁle, and not allocating any new blocks, that the bitmap will still be consulted. This is not true! Allocation structures, such as bitmaps, are only accessed when allocation is needed. The inodes, directories, and indirect blocks have all the information they need to complete a read re- quest; there is no need to make sure a block is allocated when the inode already points to it.\n\nAt some point, the ﬁle will be closed. There is much less work to be done here; clearly, the ﬁle descriptor should be deallocated, but for now, that is all the FS really needs to do. No disk I/Os take place.\n\nA depiction of this entire process is found in Figure 40.3 (time increases downward). In the ﬁgure, the open causes numerous reads to take place']","The file system locates and accesses an inode during the file read process by first finding the inode of the root directory, which is usually a well-known inode number (such as 2 in most UNIX file systems). Once the root inode is read, the file system can find pointers to data blocks containing the contents of the root directory. By following these pointers, the file system can locate the desired inode by recursively traversing the pathname until the inode is found. This process involves reading in the block containing the inode of the desired file or directory, reading its directory data, and ultimately finding the inode number needed for further operations.",simple,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
What is the role of the file server in a client/server distributed file system architecture?,"['��le sys- tem. A simple client/server distributed ﬁle system has more components than the ﬁle systems we have studied so far. On the client side, there are client applications which access ﬁles and directories through the client- side ﬁle system. A client application issues system calls to the client-side ﬁle system (such as open(), read(), write(), close(), mkdir(), etc.) in order to access ﬁles which are stored on the server. Thus, to client applications, the ﬁle system does not appear to be any different than a lo- cal (disk-based) ﬁle system, except perhaps for performance; in this way, distributed ﬁle systems provide transparent access to ﬁles, an obvious goal; after all, who would want to use a ﬁle system that required a differ- ent set of APIs or otherwise was a pain to use?\n\nThe role of the client-side ﬁle system is to execute the actions needed to service those system calls. For example, if the client issues a read() request, the client-side ﬁle system may send a message to the server-side ﬁle system (or, more commonly, the ﬁle server) to read a particular block; the ﬁle server will then read the block from disk (or its own in-memory cache), and send a message back to the client with the requested data. The client-side ﬁle system will then copy the data into the user buffer supplied to the read() system call and thus the request will complete. Note that a subsequent read() of the same block on the client may be cached in client memory or on the client’s disk even; in the best such case, no network trafﬁc need be generated.\n\nClient Application\n\nClient-side File System\n\nFile Server\n\nDisks\n\nNetworking Layer\n\nNetworking Layer\n\nFigure 48.2: Distributed File System Architecture\n\nFrom this simple overview, you should get a sense that there are two important pieces of software in a client/server distributed ﬁle system: the client-side ﬁle system and the ﬁle server. Together their behavior deter- mines the behavior of the distributed ﬁle system. Now it’s time to study one particular system: Sun’s Network File System (NFS).\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nWWW.OSTEP.ORG\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n\nASIDE: WHY SERVERS CRASH\n\nBefore getting into the details of the NFSv2 protocol, you might be wondering: why do servers crash? Well, as you might guess, there are plenty of reasons. Servers may simply suffer from a power outage (tem- porarily); only when power is restored can the machines be restarted. Servers are often comprised of hundreds of thousands or even millions of lines of code; thus, they have bugs (even good software has a few bugs per hundred or thousand lines of code), and thus they eventually will trigger a bug that will cause them to crash. They also have memory leaks; even a small memory leak will cause a system to run out of mem- ory and crash. And, ﬁnally, in distributed systems, there is a network between the client and the server; if the network acts strangely (for ex- ample, if it becomes partitioned and clients and servers are working but cannot communicate), it may appear as if a remote machine has crashed, but in reality it is just not currently reachable through the network.\n\n48.2 On To NFS\n\nOne of the earliest and quite successful distributed systems was devel- oped by Sun Microsystems, and is known as the Sun Network File Sys- tem (or NFS) [S86]. In deﬁning NFS, Sun took an unusual approach: in- stead of building a proprietary and closed system, Sun instead developed an open protocol which simply speciﬁed the exact message formats that clients and servers would use to communicate. Different groups could develop their own NFS servers and thus compete in an NFS marketplace while preserving interoperability. It worked: today there are many com- panies that sell NFS servers (including Oracle/Sun, NetApp [HLM94], EMC, IBM, and others), and the widespread success of NFS is likely at- tributed to this “open market” approach.\n\n48.3 Focus: Simple and Fast Server Crash Recovery\n\nIn this chapter, we will discuss the classic NFS protocol (version 2, a.k.a. NFSv2), which was the']","The role of the file server in a client/server distributed file system architecture is to store and manage the actual files and data that are accessed by client applications. When a client application issues a request to read or write a file, the file server retrieves the requested data from disk or cache and sends it back to the client through the client-side file system. The file server plays a crucial role in providing access to files and ensuring data integrity in a distributed file system architecture.",simple,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
"What is the main problem associated with big pages in memory allocation, as described in the context of reducing the size of page tables?","['\ndirty 0 - - - 1 - - - - - - - - - 1 1\n\nTable 20.1: A Page Table For 16-KB Address Space\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n203\n\nTHREE EASY PIECES\n\n204\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nPAGING: SMALLER TABLES\n\nThus, our hybrid approach: instead of having a single page table for the entire address space of the process, why not have one per logical seg- ment? In this example, we might thus have three page tables, one for the code, heap, and stack parts of the address space.\n\nNow, remember with segmentation, we had a base register that told us where each segment lived in physical memory, and a bound or limit register that told us the size of said segment. In our hybrid, we still have those structures in the MMU; here, we use the base not to point to the segment itself but rather to hold the physical address of the page table of that segment. The bounds register is used to indicate the end of the page table (i.e., how many valid pages it has).\n\nLet’s do a simple example to clarify. Assume a 32-bit virtual address space with 4KB pages, and an address space split into four segments. We’ll only use three segments for this example: one for code, one for heap, and one for stack.\n\nTo determine which segment an address refers to, we’ll use the top two bits of the address space. Let’s assume 00 is the unused segment, with 01 for code, 10 for the heap, and 11 for the stack. Thus, a virtual address looks like this:\n\n31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 Seg\n\nVPN\n\nOffset\n\nIn the hardware, assume that there are thus three base/bounds pairs, one each for code, heap, and stack. When a process is running, the base register for each of these segments contains the physical address of a lin- ear page table for that segment; thus, each process in the system now has three page tables associated with it. On a context switch, these registers must be changed to reﬂect the location of the page tables of the newly- running process.\n\nOn a TLB miss (assuming a hardware-managed TLB, i.e., where the hardware is responsible for handling TLB misses), the hardware uses the segment bits (SN) to determine which base and bounds pair to use. The hardware then takes the physical address therein and combines it with the VPN as follows to form the address of the page table entry (PTE):\n\nSN VPN AddressOfPTE = Base[SN] + (VPN * sizeof(PTE))\n\n= (VirtualAddress & SEG_MASK) >> SN_SHIFT = (VirtualAddress & VPN_MASK) >> VPN_SHIFT\n\nThis sequence should look familiar; it is virtually identical to what we saw before with linear page tables. The only difference, of course, is the use of one of three segment base registers instead of the single page table base register.\n\nThe critical difference in our hybrid scheme is the presence of a bounds register per segment; each bounds register holds the value of the maxi- mum valid page in the segment. For example, if the code segment is using its ﬁrst three pages (0, 1, and 2), the code segment page table will only have three entries allocated to it and the bounds register will be set\n\nWWW.OSTEP.ORG\n\nPAGING: SMALLER TABLES\n\nTIP: USE HYBRIDS When you have two good and seemingly opposing ideas, you should always see if you can combine them into a hybrid that manages to achieve the best of both worlds. Hybrid corn species, for example, are known to be more robust than any naturally-occurring species. Of course, not all hybrids are a good idea; see the Zeedonk (or Zonkey), which is a cross of a Zebra and a Donkey. If you don’t believe such a creature exists, look it up, and prepare to be amazed.\n\nto 3; memory accesses beyond the end of the segment will generate an ex- ception and likely lead to the termination of the process. In this manner, our hybrid approach realizes a signiﬁcant memory savings compared to the linear page table; unallocated pages between the stack and the heap no longer take up space in a page table (just to mark them as not valid).\n\nHowever, as you might notice,']",The answer to given question is not present in context,simple,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
What are some key concepts related to operating systems discussed in the context of the Andrew File System (AFS)?,"[' the entire ﬁle from the remote server. Thus, AFS is faster than NFS in this case by a factor of Lnet , assuming that remote access is indeed slower Ldisk than local disk. We also note that NFS in this case increases server load, which has an impact on scale as well.\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n583\n\nTHREE EASY PIECES\n\n584\n\nTHE ANDREW FILE SYSTEM (AFS)\n\nThird, we note that sequential writes (of new ﬁles) should perform similarly on both systems (Workloads 8, 9). AFS, in this case, will write the ﬁle to the local cached copy; when the ﬁle is closed, the AFS client will force the writes to the server, as per the protocol. NFS will buffer writes in client memory, perhaps forcing some blocks to the server due to client-side memory pressure, but deﬁnitely writing them to the server when the ﬁle is closed, to preserve NFS ﬂush-on-close consistency. You might think AFS would be slower here, because it writes all data to local disk. However, realize that it is writing to a local ﬁle system; those writes are ﬁrst committed to the page cache, and only later (in the background) to disk, and thus AFS reaps the beneﬁts of the client-side OS memory caching infrastructure to improve performance.\n\nFourth, we note that AFS performs worse on a sequential ﬁle over- write (Workload 10). Thus far, we have assumed that the workloads that write are also creating a new ﬁle; in this case, the ﬁle exists, and is then over-written. Overwrite can be a particularly bad case for AFS, because the client ﬁrst fetches the old ﬁle in its entirety, only to subsequently over- write it. NFS, in contrast, will simply overwrite blocks and thus avoid the initial (useless) read1.\n\nFinally, workloads that access a small subset of data within large ﬁles perform much better on NFS than AFS (Workloads 7, 11). In these cases, the AFS protocol fetches the entire ﬁle when the ﬁle is opened; unfortu- nately, only a small read or write is performed. Even worse, if the ﬁle is modiﬁed, the entire ﬁle is written back to the server, doubling the per- formance impact. NFS, as a block-based protocol, performs I/O that is proportional to the size of the read or write.\n\nOverall, we see that NFS and AFS make different assumptions and not surprisingly realize different performance outcomes as a result. Whether these differences matter is, as always, a question of workload.\n\n49.8 AFS: Other Improvements\n\nLike we saw with the introduction of Berkeley FFS (which added sym- bolic links and a number of other features), the designers of AFS took the opportunity when building their system to add a number of features that made the system easier to use and manage. For example, AFS provides a true global namespace to clients, thus ensuring that all ﬁles were named the same way on all client machines. NFS, in contrast, allows each client to mount NFS servers in any way that they please, and thus only by con- vention (and great administrative effort) would ﬁles be named similarly across clients.\n\n1We assume here that NFS reads are block-sized and block-aligned; if they were not, the NFS client would also have to read the block ﬁrst. We also assume the ﬁle was not opened with the O TRUNC ﬂag; if it had been, the initial open in AFS would not fetch the soon to be truncated ﬁle’s contents.\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nWWW.OSTEP.ORG\n\nTHE ANDREW FILE SYSTEM (AFS)\n\nASIDE: THE IMPORTANCE OF WORKLOAD One challenge of evaluating any system is the choice of workload. Be- cause computer systems are used in so many different ways, there are a large variety of workloads to choose from. How should the storage sys- tem designer decide which workloads are important, in order to make reasonable design decisions? The designers of AFS, given their experience in measuring how ﬁle sys- tems were used, made certain workload assumptions; in particular, they assumed that most ﬁles were not frequently shared, and', ' accessed sequen- tially in their entirety. Given those assumptions, the AFS design makes perfect sense. However, these assumptions are not always correct. For example, imag- ine an application that appends information, periodically, to a log. These little log writes, which add small amounts of data to an existing large ﬁle, are quite problematic for AFS. Many other difﬁcult workloads exist as well, e.g., random updates in a transaction database. One place to get some information about what types of workloads are common are through various research studies that have been performed. See any of these studies for good examples of workload analysis [B+91, H+11, R+00, V99], including the AFS retrospective [H+88].\n\nAFS also takes security seriously, and incorporates mechanisms to au- thenticate users and ensure that a set of ﬁles could be kept private if a user so desired. NFS, in contrast, had quite primitive support for security for many years.\n\nAFS also includes facilities for ﬂexible user-managed access control. Thus, when using AFS, a user has a great deal of control over who exactly can access which ﬁles. NFS, like most UNIX ﬁle systems, has much less support for this type of sharing.\n\nFinally, as mentioned before, AFS adds tools to enable simpler man- agement of servers for the administrators of the system. In thinking about system management, AFS was light years ahead of the ﬁeld.\n\n49.9 Summary\n\nAFS shows us how distributed ﬁle systems can be built quite differ- ently than what we saw with NFS. The protocol design of AFS is partic- ularly important; by minimizing server interactions (through whole-ﬁle caching and callbacks), each server can support many clients and thus reduce the number of servers needed to manage a particular site. Many other features, including the single namespace, security, and access-control lists, make AFS quite nice to use. The consistency model provided by AFS is simple to understand and reason about, and does not lead to the occa- sional weird behavior as one sometimes observes in NFS.\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n585\n\nTHREE EASY PIECES\n\n586\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nTHE ANDREW FILE SYSTEM (AFS)\n\nPerhaps unfortunately, AFS is likely on the decline. Because NFS be- came an open standard, many different vendors supported it, and, along with CIFS (the Windows-based distributed ﬁle system protocol), NFS dominates the marketplace. Although one still sees AFS installations from time to time (such as in various educational institutions, including Wisconsin), the only lasting inﬂuence will likely be from the ideas of AFS rather than the actual system itself. Indeed, NFSv4 now adds server state (e.g., an “open” protocol message), and thus bears an increasing similar- ity to the basic AFS protocol.\n\nWWW.OSTEP.ORG\n\nTHE ANDREW FILE SYSTEM (AFS)\n\nReferences\n\n[B+91] “Measurements of a Distributed File System” Mary Baker, John Hartman, Martin Kupfer, Ken Shirriff, John Ousterhout SOSP ’91, Paciﬁc Grove, CA, October 1991 An early paper measuring how people use distributed ﬁle systems. Matches much of the intuition found in AFS.\n\n[H+11] “A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications” Tyler Harter, Chris Dragga, Michael Vaughn, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau SOSP ’11, New York, NY, October 2011 Our own paper studying the behavior of Apple Desktop workloads; turns out they are a bit different than many of the server-based workloads the systems research community usually focuses upon. Also a good recent reference which points to a lot of related work.\n\n[H+88] “Scale and Performance in a Distributed File System” John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, Michael J. West ACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1, February 1988 The long journal version of the famous AFS system, still in use in a number of']",The answer to given question is not present in context,simple,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}, {'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
How does AFS global namespace benefit clients for file naming consistency over NFS?,"[' the entire ﬁle from the remote server. Thus, AFS is faster than NFS in this case by a factor of Lnet , assuming that remote access is indeed slower Ldisk than local disk. We also note that NFS in this case increases server load, which has an impact on scale as well.\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n583\n\nTHREE EASY PIECES\n\n584\n\nTHE ANDREW FILE SYSTEM (AFS)\n\nThird, we note that sequential writes (of new ﬁles) should perform similarly on both systems (Workloads 8, 9). AFS, in this case, will write the ﬁle to the local cached copy; when the ﬁle is closed, the AFS client will force the writes to the server, as per the protocol. NFS will buffer writes in client memory, perhaps forcing some blocks to the server due to client-side memory pressure, but deﬁnitely writing them to the server when the ﬁle is closed, to preserve NFS ﬂush-on-close consistency. You might think AFS would be slower here, because it writes all data to local disk. However, realize that it is writing to a local ﬁle system; those writes are ﬁrst committed to the page cache, and only later (in the background) to disk, and thus AFS reaps the beneﬁts of the client-side OS memory caching infrastructure to improve performance.\n\nFourth, we note that AFS performs worse on a sequential ﬁle over- write (Workload 10). Thus far, we have assumed that the workloads that write are also creating a new ﬁle; in this case, the ﬁle exists, and is then over-written. Overwrite can be a particularly bad case for AFS, because the client ﬁrst fetches the old ﬁle in its entirety, only to subsequently over- write it. NFS, in contrast, will simply overwrite blocks and thus avoid the initial (useless) read1.\n\nFinally, workloads that access a small subset of data within large ﬁles perform much better on NFS than AFS (Workloads 7, 11). In these cases, the AFS protocol fetches the entire ﬁle when the ﬁle is opened; unfortu- nately, only a small read or write is performed. Even worse, if the ﬁle is modiﬁed, the entire ﬁle is written back to the server, doubling the per- formance impact. NFS, as a block-based protocol, performs I/O that is proportional to the size of the read or write.\n\nOverall, we see that NFS and AFS make different assumptions and not surprisingly realize different performance outcomes as a result. Whether these differences matter is, as always, a question of workload.\n\n49.8 AFS: Other Improvements\n\nLike we saw with the introduction of Berkeley FFS (which added sym- bolic links and a number of other features), the designers of AFS took the opportunity when building their system to add a number of features that made the system easier to use and manage. For example, AFS provides a true global namespace to clients, thus ensuring that all ﬁles were named the same way on all client machines. NFS, in contrast, allows each client to mount NFS servers in any way that they please, and thus only by con- vention (and great administrative effort) would ﬁles be named similarly across clients.\n\n1We assume here that NFS reads are block-sized and block-aligned; if they were not, the NFS client would also have to read the block ﬁrst. We also assume the ﬁle was not opened with the O TRUNC ﬂag; if it had been, the initial open in AFS would not fetch the soon to be truncated ﬁle’s contents.\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nWWW.OSTEP.ORG\n\nTHE ANDREW FILE SYSTEM (AFS)\n\nASIDE: THE IMPORTANCE OF WORKLOAD One challenge of evaluating any system is the choice of workload. Be- cause computer systems are used in so many different ways, there are a large variety of workloads to choose from. How should the storage sys- tem designer decide which workloads are important, in order to make reasonable design decisions? The designers of AFS, given their experience in measuring how ﬁle sys- tems were used, made certain workload assumptions; in particular, they assumed that most ﬁles were not frequently shared, and']",The answer to given question is not present in context,reasoning,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
"How do extent-based differ from pointer-based approaches in file systems, and what additional pointers can support larger files in an imbalanced tree?","[' CONSIDER EXTENT-BASED APPROACHES A different approach is to use extents instead of pointers. An extent is simply a disk pointer plus a length (in blocks); thus, instead of requiring a pointer for every block of a ﬁle, all one needs is a pointer and a length to specify the on-disk location of a ﬁle. Just a single extent is limiting, as one may have trouble ﬁnding a contiguous chunk of on-disk free space when allocating a ﬁle. Thus, extent-based ﬁle systems often allow for more than one extent, thus giving more freedom to the ﬁle system during ﬁle allocation.\n\nIn comparing the two approaches, pointer-based approaches are the most ﬂexible but use a large amount of metadata per ﬁle (particularly for large ﬁles). Extent-based approaches are less ﬂexible but more compact; in par- ticular, they work well when there is enough free space on the disk and ﬁles can be laid out contiguously (which is the goal for virtually any ﬁle allocation policy anyhow).\n\nNot surprisingly, in such an approach, you might want to support even larger ﬁles. To do so, just add another pointer to the inode: the dou- ble indirect pointer. This pointer refers to a block that contains pointers to indirect blocks, each of which contain pointers to data blocks. A dou- ble indirect block thus adds the possibility to grow ﬁles with an additional 1024 · 1024 or 1-million 4KB blocks, in other words supporting ﬁles that are over 4GB in size. You may want even more, though, and we bet you know where this is headed: the triple indirect pointer.\n\nOverall, this imbalanced tree is referred to as the multi-level index ap- proach to pointing to ﬁle blocks. Let’s examine an example with twelve direct pointers, as well as both a single and a double indirect block. As- suming a block size of 4 KB, and 4-byte pointers, this structure can accom- modate a ﬁle of just over 4 GB in size (i.e., (12 + 1024 + 10242) × 4 KB). Can you ﬁgure out how big of a ﬁle can be handled with the addition of a triple-indirect block? (hint: pretty big)\n\nMany ﬁle systems use a multi-level index, including commonly-used ﬁle systems such as Linux ext2 [P09] and ext3, NetApp’s WAFL, as well as the original UNIX ﬁle system. Other ﬁle systems, including SGI XFS and Linux ext4, use extents instead of simple pointers; see the earlier aside for details on how extent-based schemes work (they are akin to segments in the discussion of virtual memory).\n\nYou might be wondering: why use an imbalanced tree like this? Why not a different approach? Well, as it turns out, many researchers have studied ﬁle systems and how they are used, and virtually every time they ﬁnd certain “truths” that hold across the decades. One such ﬁnding is that most ﬁles are small. This imbalanced design reﬂects such a reality; if most ﬁles are indeed small, it makes sense to optimize for this case. Thus, with a small number of direct pointers (12 is a typical number), an inode\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n467\n\nTHREE EASY PIECES\n\n468\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nFILE SYSTEM IMPLEMENTATION\n\nASIDE: LINKED-BASED APPROACHES Another simpler approach in designing inodes is to use a linked list. Thus, inside an inode, instead of having multiple pointers, you just need one, to point to the ﬁrst block of the ﬁle. To handle larger ﬁles, add an- other pointer at the end of that data block, and so on, and thus you can support large ﬁles. As you might have guessed, linked ﬁle allocation performs poorly for some workloads; think about reading the last block of a ﬁle, for example, or just doing random access. Thus, to make linked allocation work better, some systems will keep an in-memory table of link information, instead of storing the next pointers with the data blocks themselves. The table is indexed by the address of a data block D; the content of an entry is simply D�']","Extent-based approaches in file systems use extents, which are disk pointers along with a length to specify the on-disk location of a file. This approach requires fewer pointers compared to pointer-based approaches, making it more compact. However, it may limit contiguous allocation of free space. To support larger files in an imbalanced tree, additional pointers like double indirect pointers and triple indirect pointers can be used. Double indirect pointers refer to blocks containing pointers to indirect blocks, which in turn contain pointers to data blocks. This structure allows for files larger than 4GB. Triple indirect pointers further extend this capability, enabling even larger file sizes.",reasoning,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
How does the CPU scheduler decide which process runs and how to make it deterministic?,"['\n19\n\nrc, wc, (int) getpid());\n\n20\n\n21\n\n} return 0;\n\n22\n\n}\n\nFigure 5.2: p2.c: Calling fork() And wait()\n\nYou might also have noticed: the output is not deterministic. When the child process is created, there are now two active processes in the sys- tem that we care about: the parent and the child. Assuming we are run- ning on a system with a single CPU (for simplicity), then either the child or the parent might run at that point. In our example (above), the parent did and thus printed out its message ﬁrst. In other cases, the opposite might happen, as we show in this output trace:\n\nprompt> ./p1 hello world (pid:29146) hello, I am child (pid:29147) hello, I am parent of 29147 (pid:29146) prompt>\n\nThe CPU scheduler, a topic we’ll discuss in great detail soon, deter- mines which process runs at a given moment in time; because the sched- uler is complex, we cannot usually make strong assumptions about what it will choose to do, and hence which process will run ﬁrst. This non- determinism, as it turns out, leads to some interesting problems, par- ticularly in multi-threaded programs; hence, we’ll see a lot more non- determinism when we study concurrency in the second part of the book.\n\n5.2 Adding wait() System Call\n\njust created a child that prints out a message and exits. Sometimes, as it turns out, it is quite useful for a parent to wait for a child process to ﬁnish what it has been doing. This task is accomplished with the wait() system call (or its more complete sibling waitpid()); see Figure 5.2 for details.\n\nSo far, we haven’t done much:\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n37\n\nTHREE EASY PIECES\n\n38\n\nINTERLUDE: PROCESS API\n\nIn this example (p2.c), the parent process calls wait() to delay its execution until the child ﬁnishes executing. When the child is done, wait() returns to the parent.\n\nAdding a wait() call to the code above makes the output determin-\n\nistic. Can you see why? Go ahead, think about it.\n\n(waiting for you to think .... and done)\n\nNow that you have thought a bit, here is the output:\n\nprompt> ./p2 hello world (pid:29266) hello, I am child (pid:29267) hello, I am parent of 29267 (wc:29267) (pid:29266) prompt>\n\nWith this code, we now know that the child will always print ﬁrst. Why do we know that? Well, it might simply run ﬁrst, as before, and thus print before the parent. However, if the parent does happen to run ﬁrst, it will immediately call wait(); this system call won’t return until the child has run and exited2. Thus, even when the parent runs ﬁrst, it politely waits for the child to ﬁnish running, then wait() returns, and then the parent prints its message.\n\n5.3 Finally, the exec() System Call\n\nA ﬁnal and important piece of the process creation API is the exec() system call3. This system call is useful when you want to run a program that is different from the calling program. For example, calling fork() in p2.c is only useful if you want to keep running copies of the same program. However, often you want to run a different program; exec() does just that (Figure 5.3).\n\nIn this example, the child process calls execvp() in order to run the program wc, which is the word counting program. In fact, it runs wc on the source ﬁle p3.c, thus telling us how many lines, words, and bytes are found in the ﬁle:\n\nprompt> ./p3 hello world (pid:29383) hello, I am child (pid:29384) 1030 p3.c 107 hello, I am parent of 29384 (wc:29384) (pid:29383) prompt>\n\n29\n\n2There are a few cases where wait() returns before the child exits; read the man page for more details, as always. And beware of any absolute and unqualiﬁed statements this']","The CPU scheduler determines which process runs at a given moment in time. It is complex and does not allow strong assumptions about which process will run first. This non-determinism can lead to interesting problems, especially in multi-threaded programs. More non-determinism is expected when studying concurrency in the second part of the book.",reasoning,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
What function is crucial for data persistence in file systems and recovery in database management systems?,"[' lseek() to read or write from/to random parts of a ﬁle, and then reading/writing to those random parts, will indeed lead to more disk seeks. Thus, calling lseek() can certainly lead to a seek in an upcoming read or write, but absolutely does not cause any disk I/O to occur itself.\n\nwrite will begin reading from or writing to within the ﬁle. Thus, part of the abstraction of an open ﬁle is that it has a current offset, which is updated in one of two ways. The ﬁrst is when a read or write of N bytes takes place, N is added to the current offset; thus each read or write implicitly updates the offset. The second is explicitly with lseek, which changes the offset as speciﬁed above.\n\nNote that this call lseek() has nothing to do with the seek operation of a disk, which moves the disk arm. The call to lseek() simply changes the value of a variable within the kernel; when the I/O is performed, depending on where the disk head is, the disk may or may not perform an actual seek to fulﬁll the request.\n\n39.6 Writing Immediately with fsync()\n\nMost times when a program calls write(), it is just telling the ﬁle system: please write this data to persistent storage, at some point in the future. The ﬁle system, for performance reasons, will buffer such writes in memory for some time (say 5 seconds, or 30); at that later point in time, the write(s) will actually be issued to the storage device. From the perspective of the calling application, writes seem to complete quickly, and only in rare cases (e.g., the machine crashes after the write() call but before the write to disk) will data be lost.\n\nHowever, some applications require something more than this even- tual guarantee. For example, in a database management system (DBMS), development of a correct recovery protocol requires the ability to force writes to disk from time to time.\n\nTo support these types of applications, most ﬁle systems provide some additional control APIs. In the UNIX world, the interface provided to ap- plications is known as fsync(int fd). When a process calls fsync() for a particular ﬁle descriptor, the ﬁle system responds by forcing all dirty (i.e., not yet written) data to disk, for the ﬁle referred to by the speciﬁed\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n447\n\nTHREE EASY PIECES\n\n448\n\nINTERLUDE: FILE AND DIRECTORIES\n\nﬁle descriptor. The fsync() routine returns once all of these writes are complete.\n\nHere is a simple example of how to use fsync(). The code opens the ﬁle foo, writes a single chunk of data to it, and then calls fsync() to ensure the writes are forced immediately to disk. Once the fsync() returns, the application can safely move on, knowing that the data has been persisted (if fsync() is correctly implemented, that is).\n\nint fd = open(""foo"", O_CREAT | O_WRONLY | O_TRUNC); assert(fd > -1); int rc = write(fd, buffer, size); assert(rc == size); rc = fsync(fd); assert(rc == 0);\n\nInterestingly, this sequence does not guarantee everything that you might expect; in some cases, you also need to fsync() the directory that contains the ﬁle foo. Adding this step ensures not only that the ﬁle itself is on disk, but that the ﬁle, if newly created, also is durably a part of the directory. Not surprisingly, this type of detail is often overlooked, leading to many application-level bugs [P+13].\n\n39.7 Renaming Files\n\nOnce we have a ﬁle, it is sometimes useful to be able to give a ﬁle a different name. When typing at the command line, this is accomplished with mv command; in this example, the ﬁle foo is renamed bar:\n\nprompt> mv foo bar\n\nUsing strace, we can see that mv uses the system call rename(char *old, char *new), which takes precisely two arguments: the original name of the ﬁle (old) and the new name (new).\n\nOne interesting guarantee provided by the rename() call is that it is (usually) implemented as an atomic call with respect to system crashes; if the system crashes during the renaming, the ﬁle will']","The function crucial for data persistence in file systems and recovery in database management systems is fsync(). When a process calls fsync() for a particular file descriptor, the file system responds by forcing all dirty (i.e., not yet written) data to disk for the file referred to by the specified file descriptor. The fsync() routine returns once all of these writes are complete.",multi_context,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
What is the main focus of the paper on deadlock immunity in event-based concurrency?,"[' These approaches tend to be complex and hard, often more difﬁcult than using locks correctly, probably limiting their success in the real world.\n\n[J+08] “Deadlock Immunity: Enabling Systems To Defend Against Deadlocks” Horatiu Jula, Daniel Tralamazza, Cristian Zamﬁr, George Candea OSDI ’08, San Diego, CA, December 2008 An excellent recent paper on deadlocks and how to avoid getting caught in the same ones over and over again in a particular system.\n\n[K81] “Soul of a New Machine” Tracy Kidder, 1980 A must-read for any systems builder or engineer, detailing the early days of how a team inside Data General (DG), led by Tom West, worked to produce a “new machine.” Kidder’s other book are also excellent, in particular, “Mountains beyond Mountains”. Or maybe you don’t agree with me, comma?\n\n[L+08] “Learning from Mistakes – A Comprehensive Study on Real World Concurrency Bug Characteristics” Shan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou ASPLOS ’08, March 2008, Seattle, Washington The ﬁrst in-depth study of concurrency bugs in real software, and the basis for this chapter. Look at Y.Y. Zhou’s or Shan Lu’s web pages for many more interesting papers on bugs.\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n371\n\nTHREE EASY PIECES\n\nEvent-based Concurrency (Advanced)\n\nThus far, we’ve written about concurrency as if the only way to build concurrent applications is to use threads. Like many things in life, this is not completely true. Speciﬁcally, a different style of concurrent pro- gramming is often used in both GUI-based applications [O96] as well as some types of internet servers [PDZ99]. This style, known as event-based concurrency, has become popular in some modern systems, including server-side frameworks such as node.js [N13], but its roots are found in C/UNIX systems that we’ll discuss below.\n\nThe problem that event-based concurrency addresses is two-fold. The ﬁrst is that managing concurrency correctly in multi-threaded applica- tions can be challenging; as we’ve discussed, missing locks, deadlock, and other nasty problems can arise. The second is that in a multi-threaded application, the developer has little or no control over what is scheduled at a given moment in time; rather, the programmer simply creates threads and then hopes that the underlying OS schedules them in a reasonable manner across available CPUs. Given the difﬁculty of building a general- purpose scheduler that works well in all cases for all workloads, some- times the OS will schedule work in a manner that is less than optimal. The crux:\n\nTHE CRUX: HOW TO BUILD CONCURRENT SERVERS WITHOUT THREADS\n\nHow can we build a concurrent server without using threads, and thus retain control over concurrency as well as avoid some of the problems that seem to plague multi-threaded applications?\n\n33.1 The Basic Idea: An Event Loop\n\nThe basic approach we’ll use, as stated above, is called event-based concurrency. The approach is quite simple: you simply wait for some- thing (i.e., an “event”) to occur; when it does, you check what type of\n\n373\n\n33\n\n374\n\nEVENT-BASED CONCURRENCY (ADVANCED)\n\nevent it is and do the small amount of work it requires (which may in- clude issuing I/O requests, or scheduling other events for future han- dling, etc.). That’s it!\n\nBefore getting into the details, let’s ﬁrst examine what a canonical event-based server looks like. Such applications are based around a sim- ple construct known as the event loop. Pseudocode for an event loop looks like this:\n\nwhile (1) {\n\nevents = getEvents(); for (e in events)\n\nprocessEvent(e);\n\n}\n\nIt’s really that simple. The main loop simply waits for something to do (by calling getEvents() in the code above) and then, for each event re- turned, processes them, one at a time; the code that processes each event is known as an event handler. Importantly, when a handler processes an event, it is the only']",The answer to given question is not present in context,multi_context,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
"What design principle in NFSv2 ensures fast server crash recovery in a multi-client, single-server setup?","[' standard for many years; small changes were made in moving to NFSv3, and larger-scale protocol changes were made in moving to NFSv4. However, NFSv2 is both wonderful and frus- trating and thus serves as our focus.\n\nIn NFSv2, the main goal in the design of the protocol was simple and fast server crash recovery. In a multiple-client, single-server environment, this goal makes a great deal of sense; any minute that the server is down (or unavailable) makes all the client machines (and their users) unhappy and unproductive. Thus, as the server goes, so goes the entire system.\n\nc(cid:13) 2014, ARPACI-DUSSEAU\n\n561\n\nTHREE EASY PIECES\n\n562\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n\n48.4 Key To Fast Crash Recovery: Statelessness\n\nThis simple goal is realized in NFSv2 by designing what we refer to as a stateless protocol. The server, by design, does not keep track of any- thing about what is happening at each client. For example, the server does not know which clients are caching which blocks, or which ﬁles are currently open at each client, or the current ﬁle pointer position for a ﬁle, etc. Simply put, the server does not track anything about what clients are doing; rather, the protocol is designed to deliver in each protocol request all the information that is needed in order to complete the request. If it doesn’t now, this stateless approach will make more sense as we discuss the protocol in more detail below.\n\nFor an example of a stateful (not stateless) protocol, consider the open()\n\nsystem call. Given a pathname, open() returns a ﬁle descriptor (an inte- ger). This descriptor is used on subsequent read() or write() requests to access various ﬁle blocks, as in this application code (note that proper error checking of the system calls is omitted for space reasons):\n\nchar buffer[MAX]; int fd = open(""foo"", O_RDONLY); // get descriptor ""fd"" read(fd, buffer, MAX); read(fd, buffer, MAX); ... read(fd, buffer, MAX); close(fd);\n\n// read MAX bytes from foo (via fd) // read MAX bytes from foo\n\n// read MAX bytes from foo // close file\n\nFigure 48.3: Client Code: Reading From A File\n\nNow imagine that the client-side ﬁle system opens the ﬁle by sending a protocol message to the server saying “open the ﬁle ’foo’ and give me back a descriptor”. The ﬁle server then opens the ﬁle locally on its side and sends the descriptor back to the client. On subsequent reads, the client application uses that descriptor to call the read() system call; the client-side ﬁle system then passes the descriptor in a message to the ﬁle server, saying “read some bytes from the ﬁle that is referred to by the descriptor I am passing you here”.\n\nIn this example, the ﬁle descriptor is a piece of shared state between the client and the server (Ousterhout calls this distributed state [O91]). Shared state, as we hinted above, complicates crash recovery. Imagine the server crashes after the ﬁrst read completes, but before the client has issued the second one. After the server is up and running again, the client then issues the second read. Unfortunately, the server has no idea to which ﬁle fd is referring; that information was ephemeral (i.e., in memory) and thus lost when the server crashed. To handle this situa- tion, the client and server would have to engage in some kind of recovery protocol, where the client would make sure to keep enough information around in its memory to be able to tell the server what it needs to know (in this case, that ﬁle descriptor fd refers to ﬁle foo).\n\nOPERATING SYSTEMS [VERSION 0.80]\n\nWWW.OSTEP.ORG\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n\nIt gets even worse when you consider the fact that a stateful server has to deal with client crashes. Imagine, for example, a client that opens a ﬁle and then crashes. The open() uses up a ﬁle descriptor on the server; how can the server know it is OK to close a given ﬁle? In normal operation, a client would eventually call close() and thus inform the server that the ﬁle']","The main design principle in NFSv2 that ensures fast server crash recovery in a multi-client, single-server setup is the stateless protocol. In NFSv2, the server does not keep track of client-specific information such as cached blocks, open files, or file pointer positions. Instead, all necessary information is included in each protocol request, allowing for quick recovery in case of server crashes.",multi_context,"[{'source': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf', 'filename': 'data/pdfs/Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau - Operating Systems_ Three Easy Pieces-Arpaci-Dusseau (2015) (1).pdf'}]",True
